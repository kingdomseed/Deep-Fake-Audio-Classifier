# Robust Training + Prediction Distribution Report (Feb 5)

This note is meant to be shareable with a classmate. It summarizes the “robust” training run settings and how the prediction **score distributions** change across `dev`, `test1`, and the newer “beta mini test” (`test2`).

All prediction values below are sigmoid outputs in `[0, 1]` generated by `src/predict.py`.

## 1) Robust training recipe used (both models)

Same settings were used for both `cnn1d` and `cnn2d`:

- SpecAugment: `--spec-augment --time-mask-ratio 0.20 --feature-mask --feature-mask-ratio 0.10`
- Time shift: `--time-shift --time-shift-ratio 0.10`
- Channel drop: `--channel-drop --channel-drop-prob 0.05`
- Gaussian feature noise: `--gaussian-jitter --gaussian-jitter-std 0.005`
- Calibration: `--label-smoothing 0.05`
- Scheduler: `--lr-scheduler plateau --lr-scheduler-metric dev_eer`
- Early stopping: `--early-stop 8`

### Augmentation scaling sanity check (first batch)

These are the `--debug-augment-stats` prints (first training batch). The key point: values did **not** explode; variance reduced, which is expected with masking/drop.

**CNN1D**
- Before: min `-61.2030`, max `86.5541`, mean `-0.0700`, std `3.1722`, zero% `0.00`
- After:  min `-61.1941`, max `32.0381`, mean `-0.1465`, std `2.3101`, zero% `0.00`

**CNN2D**
- Before: min `-60.1494`, q01 `-10.8837`, median `-0.0002`, q99 `7.9579`, max `85.0641`, mean `-0.0701`, std `3.1942`, zero% `0.0000`
- After:  min `-60.1553`, q01 `-8.9363`, median `-0.0004`, q99 `4.5575`, max `33.0695`, mean `-0.1493`, std `2.3290`, zero% `0.0000`

## 2) Score distribution summaries

### CNN1D (`checkpoints/cnn1d_best.pt`)

**Dev** (2000 utts)
- EER: `0.000000` (from `scripts/evaluation.py`)
- Percentiles: min `0.000285`, p01 `0.000680`, p05 `0.001383`, p10 `0.001988`, p25 `0.003831`, p50 `0.017526`, p75 `0.971807`, p90 `0.986796`, p95 `0.992940`, p99 `0.998006`, max `0.999661`
- Fractions: `<0.01` = `0.448`, `>0.99` = `0.072`, in `[0.1,0.9]` = `0.049`

**Test1** (500 utts)
- Percentiles: min `0.000502`, p01 `0.000720`, p05 `0.001423`, p10 `0.002135`, p25 `0.004186`, p50 `0.012949`, p75 `0.970635`, p90 `0.985007`, p95 `0.990955`, p99 `0.996217`, max `0.998361`
- Fractions: `<0.01` = `0.460`, `>0.99` = `0.066`, in `[0.1,0.9]` = `0.036`

**Test2** (100 utts)
- Percentiles: min `0.002124`, p01 `0.002785`, p05 `0.004137`, p10 `0.006715`, p25 `0.015100`, p50 `0.041758`, p75 `0.131773`, p90 `0.392159`, p95 `0.638571`, p99 `0.902500`, max `0.915333`
- Fractions: `<0.01` = `0.160`, `>0.99` = `0.000`, in `[0.1,0.9]` = `0.260`

**Quick interpretation (cnn1d):**
- `dev` and `test1` are strongly **bi-modal** (lots near 0 and near 1). That matches very low dev EER.
- `test2` is much less confident on the high end (no `>0.99`, max ~0.915), suggesting domain shift / harder conditions.

---

### CNN2D (`checkpoints/cnn2d_best.pt`)

**Dev** (2000 utts)
- EER: `0.001005`
- Percentiles: min `0.000307`, p01 `0.005824`, p05 `0.015461`, p10 `0.024722`, p25 `0.071931`, p50 `0.507145`, p75 `0.999753`, p90 `0.999937`, p95 `0.999969`, p99 `0.999994`, max `1.000000`
- Fractions: `<0.01` = `0.029`, `>0.99` = `0.444`, in `[0.1,0.9]` = `0.234`

**Test1** (500 utts)
- Percentiles: min `0.001044`, p01 `0.007692`, p05 `0.013974`, p10 `0.028387`, p25 `0.065834`, p50 `0.387279`, p75 `0.999689`, p90 `0.999920`, p95 `0.999963`, p99 `0.999987`, max `0.999995`
- Fractions: `<0.01` = `0.022`, `>0.99` = `0.426`, in `[0.1,0.9]` = `0.254`

**Test2** (100 utts)
- Percentiles: min `0.000074`, p01 `0.000104`, p05 `0.000289`, p10 `0.000649`, p25 `0.015259`, p50 `0.105112`, p75 `0.661692`, p90 `0.879694`, p95 `0.951007`, p99 `0.980990`, max `0.991760`
- Fractions: `<0.01` = `0.220`, `>0.99` = `0.010`, in `[0.1,0.9]` = `0.410`

**Quick interpretation (cnn2d):**
- `dev`/`test1` are extremely confident on the positive end (large fraction `>0.99`).
- `test2` is noticeably flatter / shifted: median is only ~0.105, and a large fraction sits in `[0.1,0.9]`.

## 3) Cross-model comparison (what stands out)

**A) Dev is “easy” for both models**
- `cnn1d` dev EER: `0.0000`
- `cnn2d` dev EER: `0.0010`

**B) test2 looks shifted relative to dev/test1**
- For `cnn1d`, the high end collapses on test2 (max ~0.915, no >0.99).
- For `cnn2d`, test2 still produces some high scores (max ~0.992, 1% >0.99), but has many more mid-confidence cases (`[0.1,0.9]` fraction 0.410).

**C) Confidence / calibration differences**
- `cnn2d` is much more likely to output extreme high probabilities on dev/test1 (`>0.99` around ~0.43–0.44).
- `cnn1d` is bi-modal too but with far fewer `>0.99` on dev/test1 (~0.07).

## 4) Practical takeaway for final set

If the final test set is “tricky/dirty”, the key risk is domain shift:

- A model that looks perfect on dev may still degrade.
- The score distribution shift from dev/test1 → test2 is a useful warning signal.

Running both models on the final test set and comparing their distribution shift (plus possibly averaging scores / ensembling if allowed) is a straightforward strategy.
